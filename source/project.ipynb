{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROJECT - NLPCorps - CMPT825\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E-Ranked: A Deep Learning Based Product Search Relevance Tool \n",
    "#### Contributors\n",
    "- Tushar Chand Kapoor - tkapoor@sfu.ca\n",
    "- Shray Khanna - skhanna@sfu.ca\n",
    "- Manan Parasher - mparashe@sfu.ca\n",
    "____\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Going through various online retail website we found that the knowledge of NLP can be applied to improve the search results of the websites, however, to provide the best customer experience, online retailers have to give results that are relevant to the customers based on their search queries, which makes **search** an important element of the website. Providing search results to complex queries remains a challenge for many retailers\n",
    "- With increasing customers towards online purchases we decided to work upon improving the ranks of the retrieved search results.\n",
    "***\n",
    "- More formally we chose an IR (Information Retrieval) using the vector representations of the search query and the combination of the product title and product description. \n",
    "- This is done to learn the context between the query and the product description.\n",
    "- The details, working and the dataset are explained in the sections below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Baseline\n",
    "\n",
    "###### Approach 1 (XGboost)\n",
    "- In the second iteration of the baseline, we wanted to find the relevance of products using a regression approach as the problem can be funneled down to see the rank of products.\n",
    "- to get the rank of products and to see the relevancy of results we made some statistical features.\n",
    "- Feature engineering finds the common words in the search query and product description/title. It also finds the following things:\n",
    "    - Word lengths of the search term, product description, and title.\n",
    "    - Seeing search term in the product title and description\n",
    "    - Finding ratios of description to search term\n",
    "    - finding ratios of title to search term\n",
    "    - combining title and description to see ratio with search term\n",
    "- We then train these statistical features to predict the Rank of products based on search term using XGBoost.\n",
    "- The loss is calculated using RMSE on Predicted values and Truth values on Test Set.\n",
    "\n",
    "###### Approach 2 (Gensim)\n",
    "- Model Building\n",
    "    - We trained the genism word2vec model with our own custom corpus as following:\n",
    "    - Word2Vec(window = 10, sg = 1, hs = 0,\n",
    "            negative = 10, # for negative sampling\n",
    "            alpha=0.03, min_alpha=0.0007,\n",
    "            seed = 14)\n",
    "\n",
    "    - Let’s try to understand the hyperparameters of this model.\n",
    "        - size: The number of dimensions of the embeddings and the default is 100.\n",
    "        - window: The maximum distance between a target word and words around the target word. The default window is 10.\n",
    "        - sg: The training algorithm, either CBOW(0) or skip-gram(1). The default training algorithm is CBOW.\n",
    "        - hs ({0, 1}, optional) – If 1, hierarchical softmax will be used for model training. If 0, and negative is non-zero, negative sampling will be used.\n",
    "        - negative (int, optional) – If > 0, negative sampling will be used, the int for negative specifies how many “noise words” should be drawn (usually between 5-20). If set to 0, no negative sampling is used.\n",
    "        - alpha (float, optional) – The initial learning rate.\n",
    "        - min_alpha (float, optional) – Learning rate will linearly drop to min_alpha as training progresses.\n",
    "        - seed (int, optional) – Seed for the random number generator. Initial vectors for each word are seeded with a hash of the concatenation of word + str(seed).\n",
    "\n",
    "- Vocal Building:\n",
    "    - model.build_vocab(train_data, progress_per=200)\n",
    "    - To train our model on custom corpus we created our custom vocab for the model.\n",
    "\n",
    "- Model Training\n",
    "    - model.train(train_data, total_examples = model.corpus_count, epochs=10, report_delay=1)\n",
    "\n",
    "- Prediction\n",
    "    - After training the model, we used most_similar method to get the most similar products. For example model.wv.most_similar(\"100019\") [('102893', 0.9978876113891602), ('116983', 0.9978049993515015),..,('127507', 0.9976705312728882),('102263', 0.997661828994751)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Main\n",
    "- We use a convolutional-pooling methodology over the sequence of search queries and product description and title in order to learn the vector representations of the same.\n",
    "- Both search query and the product description + product title are converted into sentence embeddings:\n",
    "    - Embedding of search query:\n",
    "    $$embeds_Q = [q_1,q_2,q_3,....q_n]$$\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$where $q_n$ represents the nth word of the search query\n",
    "    - Embedding of product description and title :\n",
    "    $$embeds_P = [p_1,p_2,p_3,....p_m]$$\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$where $p_m$ represents the mth word of the product desciption + product\n",
    "\n",
    "\n",
    "- We use a contextual window [1] to caputure the  contextual structure from the search query and the product description, by starting with a temporal context window in the sequences to caputure the contenxtual features. The representation is a as follows:\n",
    "\n",
    "$$l_t = [f_{t-d}^T,..,f_{t}^T,..,f_{t+d}^T]^T,\\;\\;\\;\\;t=1,..,T$$\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$where, $f_t$ is the $t^{th}$ word and $d=\\frac{n-1}{2}$ size of the window\n",
    "\n",
    "- We have the convolution operation in which is the sliding window based abstraction, a length sequence is produced as the output from the convolution layer\n",
    "$$h_t = tanh(W_c\\cdot l_t),\\;\\;\\;\\;t=1,..,T$$\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$ where, $W_c$ is the feature transformation matrix and $tanh$ is used as the activation function\n",
    "\n",
    "\n",
    "- To retain the most useful features max pooling operation is applied to the output generated by the convolutional layers.\n",
    "$$v(i) =  \\max_{t=1,..,T}{\\{h_t(i)\\}},\\;\\;\\;\\;i=1,..,K$$\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$where, K equals the dimension of $h_t$\n",
    "\n",
    "\n",
    "- To add a non-linearity to the output tanh transformation is applied \n",
    "$$y = tanh(W_s\\cdot v),\\;\\;\\;\\;t=1,..,T$$\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$where, $W_s$ is the semantic projection matrix\n",
    "\n",
    "\n",
    "- Semantic relevance score between a search_query and product\n",
    "\n",
    "$$R(Q,P) = \\cos (y_Q,y_P) =  \\frac{y_{Q}^T y_P}{||y_Q||\\;||y_P||}$$\n",
    "\n",
    "\n",
    "- Loss Function:\n",
    "    - We use MSE (Mean Squared Error) loss between the cosine similarity values and truth reference relevances.\n",
    "        $$l(x,y) = \\{l_1,..,l_N\\}^T,l_n = (x_n-y_n)^2$$ where x and y are the tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The datasets we use for this project are the [Home Depot Product Search Relevance](https://www.kaggle.com/c/home-depot-product-search-relevance/data) which we use as the training data and for our test purposes, we use [eCommerce search relevance](https://data.world/crowdflower/ecommerce-search-relevance) dataset.\n",
    "- Both of the datasets require a series of preprocessing steps which include cleaning of the data, combining information from multiple files and web scrapping to complete the dataset.\n",
    "- The train data set is split into two the first 80% of the values are used for training and the next 20% are used as the dev set to evaluate our metric.\n",
    "- Below shown are a glimpse of train and test data. (Shown below both train and test are the clean version of the data for the raw data link is provided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import pandas as pd\n",
    "from data_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Data: [link](https://www.kaggle.com/c/home-depot-product-search-relevance/data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_uid</th>\n",
       "      <th>product_title</th>\n",
       "      <th>search_term</th>\n",
       "      <th>relevance</th>\n",
       "      <th>brand</th>\n",
       "      <th>product_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>116711</td>\n",
       "      <td>ge z wave 1800 watt resist cfl led indoor plug...</td>\n",
       "      <td>zwave switch</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>transform ani home into a smart home with the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>141628</td>\n",
       "      <td>leviton z wave control 3 way/remot scene capab...</td>\n",
       "      <td>zwave switch</td>\n",
       "      <td>3.0</td>\n",
       "      <td>leviton</td>\n",
       "      <td>the leviton dzmx1 is a z wave enabl univers di...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_uid                                      product_title  \\\n",
       "0       116711  ge z wave 1800 watt resist cfl led indoor plug...   \n",
       "1       141628  leviton z wave control 3 way/remot scene capab...   \n",
       "\n",
       "    search_term  relevance    brand  \\\n",
       "0  zwave switch        3.0      NaN   \n",
       "1  zwave switch        3.0  leviton   \n",
       "\n",
       "                                 product_description  \n",
       "0  transform ani home into a smart home with the ...  \n",
       "1  the leviton dzmx1 is a z wave enabl univers di...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('../data/train_final.csv').head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEST DATA: [link](https://data.world/crowdflower/ecommerce-search-relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_uid</th>\n",
       "      <th>relevance</th>\n",
       "      <th>product_title</th>\n",
       "      <th>search_term</th>\n",
       "      <th>rank</th>\n",
       "      <th>product_description</th>\n",
       "      <th>brand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>711158459</td>\n",
       "      <td>3.67</td>\n",
       "      <td>soni playstat 4 ps4 latest model 500 gb jet bl...</td>\n",
       "      <td>playstat 4</td>\n",
       "      <td>1</td>\n",
       "      <td>the playstat 4 system open the door to an incr...</td>\n",
       "      <td>soni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>711158460</td>\n",
       "      <td>4.00</td>\n",
       "      <td>soni playstat 4 latest model 500 gb jet black ...</td>\n",
       "      <td>playstat 4</td>\n",
       "      <td>2</td>\n",
       "      <td>the playstat 4 system open the door to an incr...</td>\n",
       "      <td>soni</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_uid  relevance                                      product_title  \\\n",
       "0    711158459       3.67  soni playstat 4 ps4 latest model 500 gb jet bl...   \n",
       "1    711158460       4.00  soni playstat 4 latest model 500 gb jet black ...   \n",
       "\n",
       "  search_term  rank                                product_description brand  \n",
       "0  playstat 4     1  the playstat 4 system open the door to an incr...  soni  \n",
       "1  playstat 4     2  the playstat 4 system open the door to an incr...  soni  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('../data/test_final.csv',encoding='ISO-8859-1').head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Although most of the code is written by the group members but few portions which were not are as follows (the source links have been commented inside the files along with the functions):\n",
    "    - def removeAdditional - To remove unnecessary characters \n",
    "    - def sliding_window - window over the sentence \n",
    "    - forward of CPSIR (taken partial implementation) - few lines are taken to complement the sliding_window function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The training was done on approximately 10k searches and queries and related products.\n",
    "- All search queries and product description and title are pre-processed in such a way that:\n",
    "    - The text was lowercased and all the special characters were removed, but the numbers were retained.\n",
    "    - The text was tokenized.\n",
    "    - Stemming was done on the cleaned text.\n",
    "    - Products relating to the same search query were stacked using torch.stack, and the texts were padded to match the stacked length.\n",
    "    - Both the search query and the description + title is processed in the same way.\n",
    "- There is no overlap amongst the train and the test data.\n",
    "- The model was trained using Stochastic Gradient Descent (SGD).\n",
    "- The performance of the model is measured by mean average precision metric shown as below:\n",
    "$$score = \\frac{ \\sum_{i=1,..,N}[\\frac{count(matches)_{P_i}}{|P_i|}] }{N}$$\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$ where, $N$ is the size of the group by search term products and $P_i$ is the size of the number of products related to current search query.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check.py \n",
    "\n",
    "- check.py scores are calculated out of 1. (Max = 1 and Min = 0)\n",
    "- It reads the output created by the program and references it against the .out files in the reference folder.\n",
    "- Every match scores a point, but if a product is missing against a particular search query or another product is present which was not part of the result we penalize it.\n",
    "- Below are the options of check.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: check.py [options]\r\n",
      "\r\n",
      "Options:\r\n",
      "  -h, --help            show this help message and exit\r\n",
      "  -d DEVFILE, --devfile=DEVFILE\r\n",
      "                        Dev File Output Path\r\n",
      "  -t TESTFILE, --testfile=TESTFILE\r\n",
      "                        Test File  Output Path\r\n",
      "  -x DEVREFFILE, --devreffile=DEVREFFILE\r\n",
      "                        Dev Reference File Path\r\n",
      "  -z TESREFFILE, --tesreffile=TESREFFILE\r\n",
      "                        Test Reference File Path\r\n",
      "  -b BASELINE, --baseline=BASELINE\r\n",
      "                        Test Only Base Line Score\r\n",
      "  -p BACKDIR, --backdir=BACKDIR\r\n",
      "                        directory to go back\r\n"
     ]
    }
   ],
   "source": [
    "!python check.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Baseline\n",
    "\n",
    "- XGBoost works by taking into account the statistical features.\n",
    "- On learning the features it predicts the relevancy.\n",
    "- It sees the words present in the search term, product description, title and brand names and evaluates the ratios and frequencies as mentioned above. \n",
    "- Running our XGBoost model on the cleaned data gives a score of 12%.\n",
    "- ***Note:*** This doesn't learn actual meaning in the description and title of the product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test score: 0.12031\r\n"
     ]
    }
   ],
   "source": [
    "!python check.py -t output/test_bs.out -p ../ -b true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Main\n",
    "\n",
    "- The model starts to learn the context with window 1.\n",
    "- After 10 epochs model learns the context and meaning in the text to find the similarity.\n",
    "- The architecture defined above helps in finding the actual similarity between search terms and product information.\n",
    "- The MSE loss drops to 0.39 after the final epoch which is in the desired range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev score: 0.44857\r\n",
      "test score: 0.38266\r\n"
     ]
    }
   ],
   "source": [
    "!python check.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files\n",
    "\n",
    "- output\n",
    "    - dev.out (dev output of main model)\n",
    "    - test.out (test output of main model)\n",
    "    - test_bs.out (test output of baseline model)\n",
    "- output/reference\n",
    "    - dev.out (dev truth values)\n",
    "    - test.out (test truth values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We see a major improvement with our Deep Learning approach to get accurate results with a score of 38.26% on the test set against 12.03% (baseline).\n",
    "- If we see both the things matching the truth values then we assign a score otherwise we penalize the model.\n",
    "- By doing so, we might not get a score of 80% and more, but it helps in providing all the results according to its context and meaning.\n",
    "\n",
    "- Inference:\n",
    "- Using the gensim model was a naive approach. We needed to make the model more accurate, henceforth we decided to move to PyTorch and neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we have implemented a deep learning approach for product ranking against a given query, even though deep learning was applied it is not a silver bullet to the solution there is, below are our thoughts on the future work:\n",
    "- Although in this iteration of the project we were able to break the rank between two products with the same similarity using cosine similarity we would like to add the following:\n",
    "    - Understanding the context of the product features based on the search query which in many scenarios is a good metric to break the tie.\n",
    "    - Given more data of user clicks, we can add that as an additional feature to determine the relevance alongside the similarity currently being produced by the model.\n",
    "- In the future, we also plan to use pre-trained embeddings coupled with our changes to enhance the performance of our model.\n",
    "- We plan to find a more diverse set of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Shen, Yelong & He, Xiaodong & Gao, Jianfeng & Deng, Li & Mesnil, Grégoire. (2014). A Latent Semantic Model with Convolutional-Pooling Structure for Information Retrieval. CIKM 2014 - Proceedings of the 2014 ACM International Conference on Information and Knowledge Management. 101-110. 10.1145/2661829.2661935.\n",
    "\n",
    "[2] Gensim: Topic Modelling for Humans. Machine Learning Consulting, Retrieved from https://radimrehurek.com/gensim/models/word2vec.html.\n",
    "\n",
    "[3] Li, Z. (2019 30). A Beginner's Guide to Word Embedding with Gensim Word2Vec Model. Retrieved from https://towardsdatascience.com/a-beginners-guide-to-word-embedding-with-gensim-word2vec-model-5970fa56cc92."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
